{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pretty_midi\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from data import generate_samples\n",
    "import tensorflow as tf\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct and train model\n",
    "x_train, y_train = generate_samples(100000, 100)\n",
    "x_test, y_test = generate_samples(10000, 10)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(84, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=10)\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempo = 120.0  # beats per min\n",
    "input_length = 3.0  # seconds\n",
    "sr = 22050  # sample rate for synthesis\n",
    "hop_length = 512\n",
    "note_velocity = 90\n",
    "min_pitch = 60  # C4\n",
    "max_pitch = 69  # A4\n",
    "num_notes = int(input_length / 60.0 * tempo)\n",
    "secs_per_beat = 60.0 / tempo\n",
    "fs = sr / hop_length  # sampling frequency of columns; time between column samples is 1./fs seconds\n",
    "C1 = 4\n",
    "C8 = 88\n",
    "steps_per_song = sr / hop_length * input_length\n",
    "secs_per_step = 1.0 / fs\n",
    "window_size = 1  # additional frame length added to either side\n",
    "assert(steps_per_song / 2 > window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate midi\n",
    "piano_midi = pretty_midi.PrettyMIDI(initial_tempo=tempo)\n",
    "piano_instr = pretty_midi.Instrument(1)\n",
    "\n",
    "# play one note per beat\n",
    "for i in range(num_notes):\n",
    "    note_start = i * secs_per_beat\n",
    "    note_end = (i+1) * secs_per_beat\n",
    "    note_pitch = np.random.randint(min_pitch, max_pitch+1)\n",
    "    note = pretty_midi.Note(note_velocity, note_pitch, note_start, note_end)\n",
    "    piano_instr.notes.append(note)\n",
    "piano_midi.instruments.append(piano_instr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate audio\n",
    "piano_audio = piano_midi.fluidsynth(fs=sr)[:int(sr*input_length)]\n",
    "print(len(piano_audio))\n",
    "ipd.Audio(piano_audio, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piano_roll = piano_midi.get_piano_roll(fs=fs)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.imshow(piano_roll, aspect='auto')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get spectrogram from audio (training input)\n",
    "C = np.abs(librosa.cqt(piano_audio, sr=sr, hop_length=hop_length, n_bins=84))\n",
    "C = C[:, :-1]  # fix off by 1 error\n",
    "print(\"C shape: {}\".format(C.shape))\n",
    "librosa.display.specshow(librosa.amplitude_to_db(C, ref=np.max),\n",
    "                         sr=sr, x_axis='time', y_axis='cqt_note')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Constant-Q power spectrum')\n",
    "plt.tight_layout()\n",
    "\n",
    "C = np.transpose(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_midi = pretty_midi.PrettyMIDI()\n",
    "piano_instr = pretty_midi.Instrument(1)\n",
    "\n",
    "print(\"new C {}\".format(C.shape))\n",
    "print(steps_per_song)\n",
    "prev_pitch = -1\n",
    "note_start = 0\n",
    "for i in range(int(steps_per_song)):\n",
    "    if i - window_size < 0 or i + window_size + 1 >= steps_per_song:\n",
    "        continue\n",
    "    my_input = np.expand_dims(C[i-window_size:i+window_size+1, :], 0)\n",
    "    note_pitch = C1 + np.argmax(model.predict(my_input))\n",
    "    if prev_pitch == -1:\n",
    "        prev_pitch = note_pitch\n",
    "    note_velocity = 90\n",
    "    if prev_pitch != note_pitch:\n",
    "        note_end = i * secs_per_step\n",
    "        note = pretty_midi.Note(note_velocity, note_pitch, note_start, note_end)\n",
    "        piano_instr.notes.append(note)\n",
    "        note_start = i * secs_per_step\n",
    "        prev_pitch = note_pitch\n",
    "reconstructed_midi.instruments.append(piano_instr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate audio\n",
    "reconstructed_audio = reconstructed_midi.fluidsynth(fs=sr)\n",
    "print(len(reconstructed_audio))\n",
    "ipd.Audio(reconstructed_audio, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
